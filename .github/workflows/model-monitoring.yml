name: Model Performance Monitoring

on:
  schedule:
    # Run daily at midnight UTC
    - cron: "0 0 * * *"

  # Allow manual triggering
  workflow_dispatch:

jobs:
  monitor-performance:
    runs-on: ubuntu-latest
    outputs:
      drift: ${{ steps.monitoring.outputs.drift }}

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Download latest production data
        run: |
          mkdir -p data/production

          # In a real environment, this would download data from a secure source
          # For demonstration, we'll generate synthetic data
          python - << 'EOL'
          import pandas as pd
          import numpy as np
          from datetime import datetime, timedelta
          import random

          # Generate synthetic data with timestamps
          np.random.seed(int(datetime.now().timestamp()))
          n_samples = 2000

          # Create feature matrix
          X = np.random.randn(n_samples, 30)

          # Create target with 5% fraud rate
          y = np.zeros(n_samples)
          fraud_indices = np.random.choice(range(n_samples), size=int(0.05*n_samples), replace=False)
          y[fraud_indices] = 1

          # Create predictions with some errors to simulate drift
          predictions = []
          for i in range(n_samples):
              if y[i] == 1:  # For fraudulent transactions
                  if random.random() < 0.7:  # 70% correct detection
                      predictions.append(1)
                  else:
                      predictions.append(0)
              else:  # For legitimate transactions
                  if random.random() < 0.97:  # 97% correct detection
                      predictions.append(0)
                  else:
                      predictions.append(1)

          # Create timestamps for last 2 days
          timestamps = []
          for i in range(n_samples):
              hours_back = random.randint(0, 48)
              timestamps.append((datetime.now() - timedelta(hours=hours_back)).isoformat())

          # Create dataframe
          columns = ['Time'] + [f'V{i}' for i in range(1, 29)] + ['Amount', 'Class']
          df = pd.DataFrame(np.column_stack([X, y]), columns=columns)
          df['predicted'] = predictions
          df['prediction_time'] = timestamps

          # Save to CSV
          df.to_csv('data/production/transactions.csv', index=False)
          print(f'Generated {n_samples} synthetic transactions with timestamps')
          EOL

          echo "Downloaded production data"

      - name: Run model monitoring script
        id: monitoring
        run: |
          mkdir -p reports
          # Run monitoring and capture exit code
          python scripts/monitor_model_performance.py --days 2 --alert-on-drift
          exit_code=$?

          # Set output based on exit code
          if [ $exit_code -eq 2 ]; then
            echo "drift=true" >> $GITHUB_OUTPUT
          else
            echo "drift=false" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Upload monitoring reports
        uses: actions/upload-artifact@v3
        with:
          name: monitoring-reports
          path: reports/

      - name: Send drift alert
        if: steps.monitoring.outputs.drift == 'true'
        run: |
          echo "MODEL DRIFT DETECTED! Sending alert..."
          # In a real environment, this would send an email or Slack notification
          # For demonstration, we just log the alert
          echo "ALERT: Model drift detected in fraud detection model" >> alerts.log
          cat reports/model_monitoring_*.txt | tail -1 >> alerts.log

  retrain-if-needed:
    needs: monitor-performance
    if: needs.monitor-performance.outputs.drift == 'true'
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Prepare for retraining
        run: |
          mkdir -p data/raw data/processed models

          # In a real environment, this would download the latest data
          # For demonstration, we create a synthetic dataset
          python - << 'EOL'
          import pandas as pd
          import numpy as np

          # Generate synthetic data for retraining
          np.random.seed(42)
          n_samples = 10000

          # Features
          X = np.random.randn(n_samples, 30)

          # Target with 5% fraud
          y = np.zeros(n_samples)
          fraud_indices = np.random.choice(range(n_samples), size=int(0.05*n_samples), replace=False)
          y[fraud_indices] = 1

          # Create DataFrame with proper column names
          columns = ['Time'] + [f'V{i}' for i in range(1, 29)] + ['Amount', 'Class']
          df = pd.DataFrame(np.column_stack([X, y]), columns=columns)

          # Save dataset
          df.to_csv('data/raw/creditcard.csv', index=False)
          print(f'Created retraining dataset with {n_samples} samples')
          EOL

          echo "Prepared for retraining"

      - name: Retrain model
        run: |
          # Set environment variables
          export ENV=development
          export ADVANCED_FEATURES=true
          export ENSEMBLE_MODELS=true

          # Run training script
          bash scripts/train_and_evaluate.sh

      - name: Verify new model performance
        run: |
          # Check if model was created and has acceptable performance
          python - << 'EOL'
          import joblib
          import os
          import sys
          # Load model metadata
          if not os.path.exists('models/model_metadata.joblib'):
              print('Model metadata not found')
              sys.exit(1)
          metadata = joblib.load('models/model_metadata.joblib')
          metrics = metadata.get('metrics', {})
          # Check F1 score - minimum acceptable is 0.75 for demo dataset
          f1 = metrics.get('f1', 0)
          print(f'New model F1 score: {f1}')
          if f1 < 0.75:
              print(f'New model performance below threshold: {f1} < 0.75')
              sys.exit(1)
          else:
              print('New model performance acceptable')
          EOL

      - name: Archive new model
        uses: actions/upload-artifact@v3
        with:
          name: new-model
          path: |
            models/best_model.joblib
            models/model_metadata.joblib

      - name: Create model update PR
        uses: peter-evans/create-pull-request@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "Update model due to performance drift"
          title: "ðŸ¤– Automated Model Update"
          body: |
            # Automated Model Update

            This PR updates the fraud detection model due to performance drift detected in production.

            ## Changes:
            - Updated `models/best_model.joblib` with retrained model
            - Updated `models/model_metadata.joblib` with new metrics

            ## Model Performance:
            - Previous F1 score: (see monitoring report)
            - New F1 score: (see model_metadata.joblib)

            Auto-generated by the model monitoring workflow.
          branch: automated-model-update
          base: development
          path: models/
          labels: automated,model-update
          draft: false
